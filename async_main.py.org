#!/usr/bin/env python

import aiohttp
import asyncio
import argparse
import feedparser
import os
import sys
import pytz
import re
import requests
import unicodedata
import warnings
from bs4 import BeautifulSoup
from datetime import datetime, timezone, timedelta
from dotenv import load_dotenv
from feedgen.feed import FeedGenerator
from feedgenerator import Rss201rev2Feed, Atom1Feed, get_tag_uri
from pathlib import Path

#from models import Service, Item
from models import Service, Item

LIMIT = 10
MAX_CONCURRENT = 5
OUTPUT_DIR   = os.path.join(os.path.dirname(__file__), "data")
OUTPUT_FNAME = "rss/{identifier}.xml"
S3_SYNC_CMD  = os.path.join(os.path.dirname(__file__), "s3_sync.py")
S3_BUCKET    = "rss"
TRUNCATE_WIDTH = 200
VERSION = 0.9

load_dotenv()

parser = argparse.ArgumentParser()
parser.add_argument("--max-concurrent", type=int, default=MAX_CONCURRENT, help="Maximum number of concurrent requests")
parser.add_argument("--limit", type=int, default=LIMIT, help="Maximum number of feed items")
parser.add_argument("--output-dir", "-o", type=str, default=OUTPUT_DIR, help="rss output dir(default: {0})".format(OUTPUT_DIR))
parser.add_argument("--output-fname", "-f", type=str, default=OUTPUT_FNAME, help="rss fname format(default: {0})".format(OUTPUT_FNAME))
parser.add_argument("--skip-scraping", action="store_true",  help="skip scrape website or parse feed")
parser.add_argument("--s3-sync", action="store_true", help="enable s3 sync")
parser.add_argument("--s3-bucket",type=str, default=S3_BUCKET,  help="s3 bucket name(default: {})".format(S3_BUCKET))
parser.add_argument("--force-generate", action="store_true",  help="force generate rss files")
parser.add_argument("--version", "-v", action="version", version="{0} {1}".format(os.path.basename(__file__), VERSION))
args = parser.parse_args()


def conv_time_struct_time_to_datetime(struct_time):

    tz = pytz.timezone("Asia/Tokyo")
    return datetime(*struct_time[:6], tzinfo=pytz.utc).astimezone(tz)

def conv_str_to_datetime(s):
    try:
        #ISO 8601形式の場合
        return datetime.fromisoformat(s)
    except ValueError:
        # 'Wed, 24 May 2023 00:00:00 +0900'
        return datetime.strptime(s, "%a, %d %b %Y %H:%M:%S %z")
    except Exception as e:
        warnings.warn(e)
        return None

def delete_html_tag(s: str) -> str:
    return re.sub(r'<.*?>', "", s)

def truncate_text(s: str, width=TRUNCATE_WIDTH, suffix="...") -> str:
    current_width = 0
    truncated = ""
    for char in s:
        # 文字の表示幅を取得（全角=2, 半角=1）
        char_width = 2 if unicodedata.east_asian_width(char) in 'FWA' else 1
        
        if current_width + char_width + len(suffix) > width:
            return truncated + suffix
        
        truncated += char
        current_width += char_width
    
    return s

async def get_news_by_bs4(session: aiohttp.ClientSession, news_url: str, **kwargs):

    try:
        async with session.get(news_url) as response:
            html = await response.text()

        soup = BeautifulSoup(html, "lxml")
        elems = soup.select(kwargs["selector"])
        news  = []

        for elem in elems:

            date  = None
            title = None
            url   = None
            description = None

            if kwargs["service"] == "conoha_wing" or kwargs["service"] == "conoha_vps":
                date_str  = elem.find("div", class_="listNewsUnit_date").text.strip()
                date  = datetime.strptime(date_str, "%Y-%m-%d")
                title = elem.find("span", class_="textLink has-arrow textColor-inherit has-noHover").text.strip()
                href  = elem.find("a", class_="listNewsUnit").get("href")
                url   = kwargs["base_url"] + href

            elif kwargs["service"] == "muumuu_news" :
                # date
                p = elem.find("p", class_="muu-section__date")
                if p is None:
                    continue
                date_str = p.text
                date  = datetime.strptime(date_str, "%Y-%m-%d")
                # title
                title = elem.find("h3", class_="muu-infomation__title").text
                # url
                href = elem.find("a", class_="muu-button muu-button--primary").get("href")
                url  = kwargs["base_url"] + href

            elif kwargs["service"] == "muumuu_campaign" :
                # date
                p = elem.find("p", class_="muu-section__date")
                if p is None:
                    continue
                date_str = p.find("span").text.strip()
                date  = datetime.strptime(date_str, "%Y-%m-%d")
                # title
                title = elem.find("h3", class_="muu-infomation__title").text
                # url
                href = elem.find("a", class_="muu-button muu-button--primary").get("href")
                url  = kwargs["base_url"] + href

            elif kwargs["service"] == "xserver" or kwargs["service"] == "xserver_business" :
                # date
                date_str = elem.dt.text
                date =  datetime.strptime(date_str, "%Y/%m/%d")
                elems2 = elem.find_all("a")
                for elem2 in elems2:
                    href  = elem2.get("href")
                url   = kwargs["base_url"] + href.replace("..", "") if kwargs["service"] == "xserver" else news_url + href
                title = elem2.text

            elif kwargs["service"] == "xdomain" or kwargs["service"] == "xserver_vps":
                date_str = elem.find("span", class_="date century").text
                date =  datetime.strptime(date_str, "%Y/%m/%d")
                a = elem.find("a", class_="hover-opacity")
                title = a.text.strip()
                url   = a.get("href")

            elif kwargs["service"] == "xserver_vps_for_game":
                date_str = elem.find("dt", class_="headlines_box_date").text
                date =  datetime.strptime(date_str, "%Y/%m/%d")
                a = elem.find("a")
                title = a.text.strip()
                url   = kwargs["base_url"] + a.get("href")
            
            elif kwargs["service"] == "lolipop" or kwargs["service"] == "lolipop_campaign" :
                date_str = elem.find("time").text.strip()
                date =  datetime.strptime(date_str, "%Y/%m/%d")
                a = elem.select("p.lol-section-top-info__title > a")[-1]
                title = a.text.strip()
                url  = kwargs["base_url"] + a.get("href")

            elif kwargs["service"] == "peraichi_info" or kwargs["service"] == "peraichi_service" :
                date_str = elem.find("span", class_=["u-color-gray-dark", "u-v-align-middle", "u-mr-sm"]).text.strip()
                m = re.search(r'(\d{4})年(\d{1,2})月(\d{1,2})日', date_str)
                if m:
                    year, month, day = map(int, m.groups())
                    date = datetime(year, month, day)
                a = elem.find("a")
                title = elem.find("div", class_=["u-fs-md", "u-mt-xs"]).text.strip()
                url  = kwargs["base_url"] + a.get("href")

            elif kwargs["service"] == "studio_info" or kwargs["service"] == "studio_update" :
                date_elem = elem.find("p", attrs={'data-date-format': True})
                if date_elem:
                    date_str = date_elem.text.strip()
                else:
                    # studioで無い場合はおそらく「もっとみる」のと番
                    continue
                date =  datetime.strptime(date_str, "%Y.%m.%d")
                a = elem.find("a")
                href = a.get("href")

                if re.search(r"^/.*", href):
                    url  = kwargs["base_url"] + href
                else:
                    url = href
                title = elem.find("h2").text.strip()
                description = elem.find("p").text.strip()
            
            if date and title and url:
                data = { "date": date, "url": url, "title": title, "description": description }
                news.append(data)
            
        return news
            
    except Exception as e:
        print("Error fetching {0} {1}".format(news_url, e))
        return []

async def get_news_by_feedparser(session: aiohttp.ClientSession, news_url: str, **kwargs):

    try:
        async with session.get(news_url) as response:
            feed_content = await response.text()
        atom = feedparser.parse(feed_content)
        news  = []

        lightsail_regex = re.compile(r'lightsail', flags=re.IGNORECASE)

        for entry in atom["entries"]:

            if "tags" in entry:
                if type(kwargs["tags"]).__name__ == "list" and len(kwargs["tags"]) > 0 and  not exists_feed_tags(entry["tags"], kwargs["tags"]):
                    continue

            title        = entry["title"]
            url          = entry["link"]
            if "description" in entry:
                description = entry["description"]
            else:
                description = None

            # lightsailの場合は、lightsailの文字列が含まれていなければskip
            if kwargs["service"] == "lightsail_news":
                m_title = lightsail_regex.search(title)
                m_desc  = lightsail_regex.search(description)
                if not m_title and not m_desc:
                    continue

            if "published_parsed" in entry :
                dt = conv_time_struct_time_to_datetime(entry["published_parsed"])
            elif "updated" in entry:
                dt = conv_str_to_datetime(entry["updated"])

            data = {"date": dt, "url": url, "title": title, "description": description }
            news.append(data)

        return news

    except Exception as e:
        print("Error parsing feed {0} {1}".format(news_url, e))
        return []


def exists_feed_tags(tags, check_tags):

    if type(check_tags).__name__ != "list":
        return False

    for check_tag in check_tags:
        for tag in tags:
            if tag["term"] == check_tag:
                return True

    return False


async def process_service(session: aiohttp.ClientSession, service_name: str, service_data: dict):
    """単一サービスを非同期で処理"""
    print(f"Processing {service_name}...")
    
    service_id = service_data["id"]
    news_url = service_data["news_url"]
    executor = service_data["executor"]
    base_url = service_data.get("base_url")
    selector = service_data.get("selector")
    tags = service_data.get("tags")

    news = []
    if executor == "bs4":
        news = await get_news_by_bs4(
            session, news_url, 
            service=service_name, 
            selector=selector, 
            base_url=base_url
        )
    elif executor == "feedparser":
        news = await get_news_by_feedparser(
            session, news_url, 
            service=service_name, 
            tags=tags
        )

    # 新しいアイテムを収集（既存の同期メソッドを使用）
    new_items = []
    for data in news:
        if not Item.exists(
            service_id=service_id, 
            title=data["title"], 
            link=data["url"], 
            pubdate=data["date"]
        ):
            new_items.append({
                "service_id": service_id, 
                "title": data["title"], 
                "link": data["url"], 
                "pubdate": data["date"],
                "description": data.get("description")
            })
    
    print(f"{service_name} parser end. found {len(new_items)} new items")
    return service_name, new_items

async def run_sync_s3_command():
    """基本的なコマンド実行"""
    cmds = [ S3_SYNC_CMD, "--local-sync-dir", args.output_dir, "--s3-bucket", args.s3_bucket ]
    #if "AWS_PROFILE" in os.environ:
    #    cmds.extend(["--aws-profile", os.environ["AWS_PROFILE"]])
    process = await asyncio.create_subprocess_exec(
        *cmds,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE
    )
    
    stdout, stderr = await process.communicate()
    
    return {
        'returncode': process.returncode,
        'stdout': stdout.decode('utf-8'),
        'stderr': stderr.decode('utf-8')
    }


async def write_feed_file_async(fname: str, content: str):
    """ファイル書き込みを非同期で実行"""
    loop = asyncio.get_event_loop()
    await loop.run_in_executor(None, _write_feed_file_sync, fname, content)

def _write_feed_file_sync(fname: str, content: str):
    """同期的なファイル書き込み"""
    with open(fname, "w", encoding="utf-8") as f:
        f.write(content)

async def generate_feed_async(service_name: str, service_data: dict, updated_count: int, force_generate: bool = False, limit: int = LIMIT):
    """RSSフィードを非同期で生成"""
    if updated_count == 0 and not force_generate:
        return
        
    print(f"Generating feed for {service_name}...")
    
    service_id = service_data["id"]
    category_name = service_data["category_name"]
    s3_key = args.output_fname.format(identifier=service_name, id=service_id, category_name=category_name)
    fname = os.path.join(args.output_dir, s3_key)
    output_dir = os.path.dirname(fname)
    
    # ディレクトリ作成（同期的に実行）
    if not os.path.exists(output_dir):
        Path(output_dir).mkdir(parents=True, exist_ok=True)

    # アイテム取得（既存の同期メソッドを使用）
    items = Item.get_items_by_service_id(service_id, limit=limit)
    
    # RSS フィード生成
    fg = FeedGenerator()
    fg.title(service_data["name"])
    fg.link(href=service_data["news_url"], rel="alternate") 
    fg.description("feed")

    for item in items:

        fe = fg.add_entry(order="append")
        fe.title(item.title)
        fe.link(href=item.link)
        if item.description:
            fe.description(truncate_text(delete_html_tag(item.description)))
            fe.content(item.description, type="html")
        else:
            #fe.description("No description")
            pass
        fe.published(item.pubdate.replace(tzinfo=timezone.utc))
        fe.guid(item.link, permalink=True)


    # XML生成（CPU集約的な処理なので、スレッドプールで実行）
    #loop = asyncio.get_event_loop()
    #feed_str = await loop.run_in_executor(
    #    None, 
    #    lambda: xml.dom.minidom.parseString(feed.writeString("utf-8")).toprettyxml(indent="  ")
    #)
    #feed_str = xml.dom.minidom.parseString(feed.writeString("utf-8")).toprettyxml(indent="  ")
    feed_content = fg.rss_str(pretty=True)
    
    # ファイル書き込み（非同期）
    await write_feed_file_async(fname, feed_content.decode("utf-8"))
    print(f"Generated feed for {service_name}: {fname}")

async def main():

    services = Service.get_all_services()

    timeout = aiohttp.ClientTimeout(total=30)
    connector = aiohttp.TCPConnector(limit=args.max_concurrent)

    async with aiohttp.ClientSession(
        timeout=timeout,
        connector=connector,
        headers={
            'User-Agent': 'Mozilla/5.0 (compatible; NewsAggregator/1.0)'
        }
    ) as session:

        all_items = []
        updates = {}
        if args.skip_scraping:
            print(">> Website/Feed parse skip")
        else:
            print(">> Website/Feed parse start")
            tasks = []
            for service_name, service_data in services.items():
                task = process_service(session, service_name, service_data)
                tasks.append(task)

            # セマフォで同時実行数を制限
            semaphore = asyncio.Semaphore(args.max_concurrent)
            async def limited_process(task):
                async with semaphore:
                    return await task
            results = await asyncio.gather(*[limited_process(task) for task in tasks])
    
            print(">> Insert parsed items start")
            for service_name, new_items in results:
                updates[service_name] = len(new_items)
                all_items.extend(new_items)

            if len(all_items) > 0:
                Item.bulk_insert(all_items)
                print("Insert parsed items end. inserted {0} items".format(len(all_items)))
            else:
                print("No new items")

        print("")

    print(">> Make and output feed start")
    feed_tasks = []
    for service_name, service_data in services.items():
        updated_count = updates[service_name] if service_name in updates else 0
        feed_task = generate_feed_async(
                                service_name=service_name,
                                service_data=service_data,
                                updated_count=updated_count,
                                force_generate=args.force_generate,
                                limit=args.limit
                            ) 
        feed_tasks.append(feed_task) 
    await asyncio.gather(*feed_tasks)
    print("Feed generation completed")
    print("")

    if args.s3_sync:
        print(">> Syncing output feed to Sync S3 bucket start")
        result = await run_sync_s3_command()
        print(f"Return code: {result['returncode']}")
        print(f"Output: {result['stdout']}")
        if result['stderr']:
            print(f"Error: {result['stderr']}")
        print("Syncing s3 completed")
        print("")


    print("\n=== Summary ===")
    total_updates = sum(updates.values())
    print("Total services processed: {0}".format(len(services)))
    print("Total new items found: {0}".format(total_updates))
    for service_name, count in updates.items():
        if count > 0:
            print("  {0}: {1} new items".format(service_name, count))

if __name__ == "__main__":
    asyncio.run(main())

